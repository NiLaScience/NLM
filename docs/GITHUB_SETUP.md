# NLM GitHub Setup Guide

This repository contains a minimal implementation of a GPT-style language model. Due to file size limitations on GitHub, some files are not included and need to be downloaded separately.

## ğŸš€ Quick Start

1. Clone and install
```bash
git clone https://github.com/yourusername/NLM.git
cd NLM
pip install -e .[viz]
```

2. One-command pipeline
```bash
nlm-run
# or include ancients
nlm-run --with_ancients --ancient_share 0.4 --flash_attention
```

## ğŸ“ Repository Structure

```
NLM/
â”œâ”€â”€ min_lm/                 # Core library code
â”‚   â”œâ”€â”€ tokenization/       # BPE tokenizer
â”‚   â””â”€â”€ transformer/        # Model components
â”œâ”€â”€ ancients/               # Ancient texts preparation utilities
â”œâ”€â”€ examples/               # Example scripts
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ run_end_to_end.py       # End-to-end orchestrator (exposed as nlm-run)
â””â”€â”€ README.md               # Main documentation
```

## ğŸ—„ï¸ Large Files (Not in Git)

Excluded by .gitignore:

- `data/` - Raw text datasets
- `tokenized_data/` - Preprocessed token arrays
- `checkpoints/` - Model checkpoints
- `outputs/` - Generated text and logs

## ğŸ¯ Low-level Usage Examples

```bash
# Train tokenizer
python -m NLM.min_lm.scripts.train_tokenizer data/train.txt --vocab_size 10000

# Tokenize
python -m NLM.min_lm.scripts.tokenize --inputs data/train.txt data/valid.txt \
  --vocab_path tokenizers/train_vocab.json --merges_path tokenizers/train_merges.txt

# Train
python -m NLM.min_lm.scripts.train --train_tokens tokenized_data/train.npy \
  --val_tokens tokenized_data/valid.npy --vocab_size 10000
```

## ğŸ”„ Reproducing TinyStories

```bash
nlm-run
```

Expected training time varies by hardware.

## ğŸ“Š Pre-trained Resources

The repo includes a pre-trained TinyStories tokenizer:
- Vocabulary size: 10,000 tokens
- Special tokens: `<|endoftext|>`

## ğŸ¤ Contributing

Issues and PRs welcome.

## ğŸ“ License

MIT



